"""
    Paper: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows
    Link: https://arxiv.org/abs/2103.14030
"""

import torch
import torch.nn as nn
from utils import window_reverse, window_partition
import torch.nn.functional as F
import numpy as np
from timm.models.layers import trunc_normal_

class ShiftedWindowAttention(nn.Module):
    r""" Window based multi-head self attention (W-MSA) module with relative position bias.
    It supports both of shifted and non-shifted window.

    Args:
        dim (int): Number of input channels.
        window_size (tuple[int]): The height and width of the window.
        num_heads (int): Number of attention heads.
        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set
        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0
    